{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4937d5e8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2c3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921274f",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.common_fc = nn.Sequential(\n",
    "            nn.Linear(28*28, out_features=196), nn.Tanh(),\n",
    "            nn.Linear(196, out_features=48), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.mean_fc = nn.Sequential(\n",
    "            nn.Linear(48, out_features=16), nn.Tanh(),\n",
    "            nn.Linear(16, out_features=2), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.log_var_fc = nn.Sequential(\n",
    "            nn.Linear(48, out_features=16), nn.Tanh(),\n",
    "            nn.Linear(16, out_features=2), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.decoder_fcs = nn.Sequential(\n",
    "            nn.Linear(2, out_features=16), nn.Tanh(), \n",
    "            nn.Linear(16, out_features=48), nn.Tanh(),\n",
    "            nn.Linear(48, out_features=196), nn.Tanh(),\n",
    "            nn.Linear(196, out_features=28*28), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def encode(self,x):\n",
    "        # B,C,H,W\n",
    "        out = self.common_fc(torch.flatten(x, start_dim=-1))\n",
    "        mean = self.mean_fc(out)\n",
    "        log_var = self.log_var_fc(out)\n",
    "        return mean, log_var\n",
    "\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        std = torch.exp(0.5*torch.flatten(log_var, start_dim=-1))\n",
    "        z = torch.randn_like(torch.flatten(std, start_dim=-1))\n",
    "        return z * std + mean\n",
    "    \n",
    "    \n",
    "    def decode(self, z):\n",
    "        flat_z = torch.flatten(z, start_dim=-1)\n",
    "        out = self.decoder_fcs(flat_z)\n",
    "        out = torch.reshape(out, [1, 28*28])\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        #B,C,H,W\n",
    "        outputs = []\n",
    "        for sample in batch_x:\n",
    "            #Encoder\n",
    "            mean, log_var = self.encode(torch.flatten(sample))\n",
    "            #Sampling\n",
    "            z = self.sample(mean,log_var)\n",
    "            #Decoder\n",
    "            outputs.append(self.decode(z))\n",
    "\n",
    "        out = torch.stack(outputs, dim=1)\n",
    "        return mean, log_var, out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521d7c5",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc24d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device init\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "LEARN_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfcdb8",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "def create_data():\n",
    "    # ---INITIALIZE DATASET ---\n",
    "    #Convert pilimage dataset to a standart numpy dataset\n",
    "    dataset = MNIST(\n",
    "        root='./data',\n",
    "        download=True,  # Add this to download the dataset if needed\n",
    "        transform= transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    TRIM_LEN = int(50_000)  # 60,000 - 50,000 = 10,000 SAMPLES\n",
    "    TRAIN_PORTION = 0.9 # 90% training 10% everything else\n",
    "    TRAIN_LEN = int((len(dataset) - TRIM_LEN) * TRAIN_PORTION)\n",
    "    \n",
    "    # ---SPLIT DATASET---\n",
    "    train_ds, test_ds, _ = random_split(\n",
    "        dataset,  # Split the dataset, not the dataloader!\n",
    "        [TRAIN_LEN, len(dataset) - TRIM_LEN - TRAIN_LEN, TRIM_LEN]\n",
    "    )\n",
    "    #print(f\"train length: {len(train_ds)} test_length: {len(test_ds)}\")\n",
    "    \n",
    "    # ---CREATE DATALOADERS from the split datasets---\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6dbc42",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d37c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, optimizer):\n",
    "    # Method 1: One-liner\n",
    "    #single_batch = next(iter(train_loader))[0].to(device)\n",
    "\n",
    "    for single_batch in train_loader:\n",
    "        single_batch = single_batch[0].to(device)\n",
    "        # ---------Feed Forward---------\n",
    "        # Extract just the generated images for now\n",
    "        _,_,img_gen_batch = model.forward(single_batch)\n",
    "\n",
    "\n",
    "        #---------Back Prop---------\n",
    "        # Loss is calculated by the batch's mean\n",
    "        loss = 0\n",
    "        kl_loss = torch.nn.KLDivLoss(reduce='batchmean')\n",
    "        \n",
    "        flat_sample = torch.flatten(single_batch,start_dim=1)\n",
    "        loss = kl_loss(F.log_softmax(img_gen_batch),\n",
    "                        flat_sample)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafa64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "C:\\Users\\orian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\orian\\AppData\\Local\\Temp\\ipykernel_42676\\2710401959.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = kl_loss(F.log_softmax(img_gen_batch),\n",
      "C:\\Users\\orian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0297, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0284, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0257, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0278, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0280, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0285, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0275, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0276, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0318, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0254, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0274, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0282, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0298, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0271, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0298, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0294, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0294, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0255, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0264, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0302, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0263, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0275, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0294, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0279, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0268, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0294, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0273, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0264, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0313, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0257, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0242, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0287, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0277, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0297, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0271, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0246, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0286, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0289, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0295, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0289, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0265, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0304, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0266, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0277, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0316, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0292, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0283, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0263, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0267, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0273, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0284, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0286, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0294, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0276, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0261, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0310, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0312, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0288, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0254, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0298, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0300, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0276, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0279, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0282, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0291, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0289, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0260, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0277, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0270, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0257, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0287, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0296, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0270, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0251, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0265, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0263, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0274, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0250, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0274, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0274, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0276, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0291, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0278, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0282, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0298, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0269, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0284, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0256, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0295, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0304, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0252, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0292, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0280, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0289, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0283, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0286, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0312, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0291, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0280, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0281, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0255, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0300, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "my_vae = VAE().to(device)\n",
    "optim = torch.optim.Adam(params=my_vae.parameters(),lr = LEARN_RATE)\n",
    "\n",
    "\n",
    "train_loader,test_loader = create_data()\n",
    "train_vae(model=my_vae, train_loader=train_loader,\n",
    "          optimizer=optim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
