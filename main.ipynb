{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4937d5e8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de2c3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921274f",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c50f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.common_fc = nn.Sequential(\n",
    "            nn.Linear(28*28, out_features=196), nn.Tanh(),\n",
    "            nn.Linear(196, out_features=48), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.mean_fc = nn.Sequential(\n",
    "            nn.Linear(48, out_features=16), nn.Tanh(),\n",
    "            nn.Linear(16, out_features=2), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.log_var_fc = nn.Sequential(\n",
    "            nn.Linear(48, out_features=16), nn.Tanh(),\n",
    "            nn.Linear(16, out_features=2), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.decoder_fcs = nn.Sequential(\n",
    "            nn.Linear(2, out_features=16), nn.Tanh(), \n",
    "            nn.Linear(16, out_features=48), nn.Tanh(),\n",
    "            nn.Linear(48, out_features=196), nn.Tanh(),\n",
    "            nn.Linear(196, out_features=28*28), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def encode(self,x):\n",
    "        # B,C,H,W\n",
    "        out = self.common_fc(torch.flatten(x, start_dim=-1))\n",
    "        mean = self.mean_fc(out)\n",
    "        log_var = self.log_var_fc(out)\n",
    "        return mean, log_var\n",
    "\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        std = torch.exp(0.5*torch.flatten(log_var, start_dim=-1))\n",
    "        z = torch.randn_like(torch.flatten(std, start_dim=-1))\n",
    "        return z * std + mean\n",
    "    \n",
    "    \n",
    "    def decode(self, z):\n",
    "        flat_z = torch.flatten(z, start_dim=-1)\n",
    "        out = self.decoder_fcs(flat_z)\n",
    "        out = torch.reshape(out, [1, 28*28])\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_x):\n",
    "        #B,C,H,W\n",
    "        outputs = []\n",
    "        for sample in batch_x:\n",
    "            #Encoder\n",
    "            mean, log_var = self.encode(torch.flatten(sample))\n",
    "            #Sampling\n",
    "            z = self.sample(mean,log_var)\n",
    "            #Decoder\n",
    "            outputs.append(self.decode(z))\n",
    "\n",
    "        out = torch.stack(outputs, dim=1)\n",
    "        return mean, log_var, out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521d7c5",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc24d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device init\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "LEARN_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfcdb8",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3f6d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "def create_data():\n",
    "    # ---INITIALIZE DATASET ---\n",
    "    #Convert pilimage dataset to a standart numpy dataset\n",
    "    dataset = MNIST(\n",
    "        root='./data',\n",
    "        download=True,  # Add this to download the dataset if needed\n",
    "        transform= transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    TRIM_LEN = int(58_000)  # 60,000 - 58,000 = 2,000 SAMPLES\n",
    "    TRAIN_PORTION = 0.9 # 90% training 10% everything else\n",
    "    TRAIN_LEN = int((len(dataset) - TRIM_LEN) * TRAIN_PORTION)\n",
    "    \n",
    "    # ---SPLIT DATASET---\n",
    "    train_ds, test_ds, _ = random_split(\n",
    "        dataset,  # Split the dataset, not the dataloader!\n",
    "        [TRAIN_LEN, len(dataset) - TRIM_LEN - TRAIN_LEN, TRIM_LEN]\n",
    "    )\n",
    "    #print(f\"train length: {len(train_ds)} test_length: {len(test_ds)}\")\n",
    "    \n",
    "    # ---CREATE DATALOADERS from the split datasets---\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6dbc42",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader):\n",
    "    # Method 1: One-liner\n",
    "    single_batch = next(iter(train_loader))[0].to(device)\n",
    "\n",
    "\n",
    "    # ---------Feed Forward---------\n",
    "\n",
    "    # Extract just the generated images for now\n",
    "    _,_,img_gen_batch = model.forward(single_batch)\n",
    "\n",
    "\n",
    "    #---------Back Prop---------\n",
    "\n",
    "    # Accumulate model loss\n",
    "    for ds_sample, img_gen in zip(single_batch, img_gen_batch[0]):\n",
    "        flat_sample = torch.flatten(ds_sample)\n",
    "        torch.kl_div(img_gen, flat_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dafa64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n",
      "single_batch's shape:  torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "my_vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(params=my_vae.parameters(),lr = LEARN_RATE)\n",
    "\n",
    "\n",
    "train_loader,test_loader = create_data()\n",
    "train_vae(model=my_vae, train_loader=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
